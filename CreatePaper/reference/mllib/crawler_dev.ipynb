{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T17:18:02.585074Z",
     "start_time": "2021-09-29T17:16:10.866645Z"
    },
    "execution": {
     "iopub.execute_input": "2021-11-09T14:47:21.387021Z",
     "iopub.status.busy": "2021-11-09T14:47:21.386642Z",
     "iopub.status.idle": "2021-11-09T15:08:28.369150Z",
     "shell.execute_reply": "2021-11-09T15:08:28.368502Z",
     "shell.execute_reply.started": "2021-11-09T14:47:21.386940Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Web page..:   0%|          | 0/199 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yoo/opt/anaconda3/bin/python\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Web page..: 100%|██████████| 199/199 [04:49<00:00,  1.46s/it]\n",
      "Extracting News url..: 100%|██████████| 796/796 [00:00<00:00, 32129.44it/s]\n",
      "Crawling News article..: 100%|██████████| 796/796 [16:16<00:00,  1.23s/it]\n",
      "Crawling News datetime..: 100%|██████████| 796/796 [00:00<00:00, 182023.01it/s]\n",
      "processing complie 1: 100%|██████████| 796/796 [00:00<00:00, 8751.10it/s]\n",
      "processing complie 2: 100%|██████████| 796/796 [00:00<00:00, 33390.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make DataFrame Structure\n",
      "Done\n",
      "CPU times: user 1min 36s, sys: 3.29 s, total: 1min 39s\n",
      "Wall time: 21min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "import sys\n",
    "print(sys.executable)\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "from requests import Request, Session\n",
    "from requests.exceptions import ConnectionError, Timeout, TooManyRedirects\n",
    "# from airflow.operators.python_operator import BranchPythonOperator\n",
    "# from airflow.operators.bash_operator import BashOperator\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "# from airflow.contrib.hooks.aws_hook import AwsHook\n",
    "# from airflow.models import Variable\n",
    "# from airflow.hooks.S3_hook import S3Hook\n",
    "# import boto3\n",
    "import gzip\n",
    "import os\n",
    "from io import StringIO\n",
    "import time\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "### Use list ###\n",
    "basic_date_list = []\n",
    "basic_news_article = []\n",
    "depth_date_list = []\n",
    "depth_news_article = []\n",
    "clear_word = []\n",
    "\n",
    "\n",
    "### Crawler 1 ###\n",
    "def basic_crawler():\n",
    "    \"\"\"\n",
    "    Crawler(data gathering)\n",
    "    \"\"\"\n",
    "    d = []\n",
    "    a = []\n",
    "\n",
    "    for i in tqdm(range(1,200), desc=\"Extracting Web page..\"):\n",
    "        url = 'https://www.coindeskkorea.com/news/articleList.html?page={}&total=6048&box_idxno=&view_type=sm'.format(i)\n",
    "        resp = requests.get(url)\n",
    "        soup = BeautifulSoup(resp.content, 'lxml')\n",
    "        time.sleep(0.01)\n",
    "\n",
    "        for j in range(1,21):\n",
    "            date = soup.select('#user-container div.float-center.custom-m.mobile.template.list.max-width-1250 div.user-content section article div.article-list section div:nth-of-type({}) div.text-block div.list-dated'.format(j))\n",
    "            article = soup.select(\n",
    "            '#user-container div.float-center.custom-m.mobile.template.list.max-width-1250 div.user-content section article div.article-list section div:nth-of-type({}) div.text-block p a'.format(\n",
    "                j))\n",
    "\n",
    "            d.append(date)\n",
    "            a.append(article)\n",
    "\n",
    "    for _ in d:\n",
    "        if _:\n",
    "            basic_date_list.append(_)\n",
    "\n",
    "    for _ in a:\n",
    "        if _:\n",
    "            basic_news_article.append(_)\n",
    "\n",
    "    return basic_date_list, basic_news_article\n",
    "\n",
    "\n",
    "### Crawler 2 ###\n",
    "def depth_crawler():\n",
    "    \"\"\"\n",
    "    Optimization basic_crawl data\n",
    "    \"\"\"\n",
    "    # depth_news_article\n",
    "    processing_news = []\n",
    "    for i in tqdm(basic_news_article, desc=\"Extracting News url..\"):\n",
    "        url = 'https://www.coindeskkorea.com'+str(i).split(' ')[2].split('\"')[1]\n",
    "\n",
    "        processing_news.append(url)\n",
    "\n",
    "    for i in tqdm(processing_news, desc=\"Crawling News article..\"):\n",
    "        resq = requests.get(i)\n",
    "        soup = BeautifulSoup(resq.text,'lxml')\n",
    "        n = str(soup.find_all('p')).split('<p><a href=')[0]\n",
    "\n",
    "        depth_news_article.append(n)\n",
    "\n",
    "    # depth_date_list\n",
    "    for i in tqdm(basic_date_list, desc=\"Crawling News datetime..\"):\n",
    "        d = i[0].text.split(' ')[-2]\n",
    "\n",
    "        depth_date_list.append(d)\n",
    "\n",
    "    return depth_news_article, depth_date_list\n",
    "\n",
    "\n",
    "### pre-processing ###\n",
    "def processing():\n",
    "    \"\"\"\n",
    "    Article pre-processing\n",
    "    \"\"\"\n",
    "    process = []\n",
    "    for i in tqdm(depth_news_article, desc=\"processing complie 1\"):\n",
    "        a = re.compile('[가-힣]+').findall(i)\n",
    "\n",
    "        process.append(a)\n",
    "\n",
    "    for i in tqdm(process, desc=\"processing complie 2\"):\n",
    "        a = ' '.join(i)\n",
    "        b = re.compile('[제보-보내주세요]').sub(' ',a)\n",
    "        # c = b.replace('도자료','')\n",
    "\n",
    "        clear_word.append(' '+b)\n",
    "\n",
    "    return clear_word\n",
    "\n",
    "\n",
    "### dataframe ###\n",
    "def dataframe():\n",
    "    df_article = pd.DataFrame(clear_word, columns=['article'])\n",
    "    df_date = pd.DataFrame(depth_date_list, columns=['created_date'])\n",
    "    df_total = pd.concat([df_date, df_article], axis=1)\n",
    "    df_total = df_total.groupby('created_date').sum(str(df_total['article']))\n",
    "    print(\"Make DataFrame Structure\")\n",
    "\n",
    "    return df_total.to_csv('/Users/yoo/Data-dev/nlp/reference/DataFrame/article_comparison.csv', encoding='utf-8'), df_total.to_csv('/Users/yoo/Data-dev/nlp/dev/article_comparison.csv', encoding='utf-8')\n",
    "    # return df_total\n",
    "\n",
    "\n",
    "# ### Upload S3 bucket ###\n",
    "# def DataFrameUploader(df_total):\n",
    "#     s3_bucket = 'russo-mydata'\n",
    "#     s3_key = 'article_{{ tomorrow_ds_nodash }}.csv'\n",
    "#     s3_hook = S3Hook(bucket)\n",
    "#\n",
    "#     return df_total.to_csv\n",
    "\n",
    "\n",
    "\n",
    "### main_compile ###\n",
    "basic_crawler()\n",
    "depth_crawler()\n",
    "processing()\n",
    "dataframe()\n",
    "# DataFrameUploader(dataframe())\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-08T13:59:26.548252Z",
     "iopub.status.busy": "2021-11-08T13:59:26.547840Z",
     "iopub.status.idle": "2021-11-08T13:59:26.591297Z",
     "shell.execute_reply": "2021-11-08T13:59:26.589248Z",
     "shell.execute_reply.started": "2021-11-08T13:59:26.548215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.3 ms, sys: 4.46 ms, total: 18.7 ms\n",
      "Wall time: 16.6 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_date</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-04-19</td>\n",
       "      <td>도자료는     도자료는 으로           도자료는</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-04-20</td>\n",
       "      <td>도자료는 으로</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-04-24</td>\n",
       "      <td>도자료는     도자료는     도자료는 으로</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-04-25</td>\n",
       "      <td>도자료는</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-04-26</td>\n",
       "      <td>도자료는 으로</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>2020-06-04</td>\n",
       "      <td>짧은뉴스 오늘 을 통해 국 외 업계 이모저모를 전달해드립니다  도자료는 코인데스크...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>2020-06-09</td>\n",
       "      <td>지닥 데이빗 코인원 등 최근 카카오 계열 암호화폐인 클레이 를 원화마켓에 상장했던...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>2020-06-10</td>\n",
       "      <td>미국 전역에서 조지 플로이드 씨 사망으로 촉발된 시위가 이어지면서  석금이 없어 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>2020-06-13</td>\n",
       "      <td>김태원 글로스퍼 대표가 일 사망한 것으로 확인됐다 김 대표가 부회장을 맡고 있는 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>2020-06-15</td>\n",
       "      <td>월가여 두려워 말라 지난   식시장의 매서운 매도 에도 불구하고 일 오전 지수는 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>332 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    created_date                                            article\n",
       "0     2018-04-19                    도자료는     도자료는 으로           도자료는\n",
       "1     2018-04-20                                      도자료는 으로      \n",
       "2     2018-04-24                    도자료는     도자료는     도자료는 으로      \n",
       "3     2018-04-25                                               도자료는\n",
       "4     2018-04-26                                      도자료는 으로      \n",
       "..           ...                                                ...\n",
       "327   2020-06-04   짧은뉴스 오늘 을 통해 국 외 업계 이모저모를 전달해드립니다  도자료는 코인데스크...\n",
       "328   2020-06-09   지닥 데이빗 코인원 등 최근 카카오 계열 암호화폐인 클레이 를 원화마켓에 상장했던...\n",
       "329   2020-06-10   미국 전역에서 조지 플로이드 씨 사망으로 촉발된 시위가 이어지면서  석금이 없어 ...\n",
       "330   2020-06-13   김태원 글로스퍼 대표가 일 사망한 것으로 확인됐다 김 대표가 부회장을 맡고 있는 ...\n",
       "331   2020-06-15   월가여 두려워 말라 지난   식시장의 매서운 매도 에도 불구하고 일 오전 지수는 ...\n",
       "\n",
       "[332 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "article = pd.read_csv('/Users/yoo/Data-dev/nlp/dev/article.csv')\n",
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Price crawler\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "#####################\n",
    "### import module ###\n",
    "#####################\n",
    "from requests import Request, Session\n",
    "from requests.exceptions import ConnectionError, Timeout, TooManyRedirects\n",
    "from airflow.operators.python_operator import BranchPythonOperator\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from airflow.contrib.hooks.aws_hook import AwsHook\n",
    "from airflow.models import Variable\n",
    "from airflow.hooks.S3_hook import S3Hook\n",
    "import gzip\n",
    "import logging\n",
    "import os\n",
    "from io import StringIO\n",
    "import boto3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "#####################\n",
    "### Pandas Option ###\n",
    "#####################\n",
    "pd.options.display.float_format = '{:.5f}'.format\n",
    "\n",
    "\n",
    "\n",
    "############################\n",
    "### S3 bucket Parameters ###\n",
    "############################\n",
    "s3_sensorflag_bucket = 'russo-mydata'\n",
    "s3_conn_id = 'russo-mydata'\n",
    "s3_bucket = 'russo-mydata'\n",
    "\n",
    "\n",
    "#######################\n",
    "### Bithumb Crawler ###\n",
    "#######################\n",
    "def bithumb_crawler():\n",
    "    \"\"\"\n",
    "    Bithumb Crawl during 24H information\n",
    "    \"\"\"\n",
    "    bithumb_url = 'https://api.bithumb.com/public/ticker/ALL_KRW'\n",
    "    html = requests.get(bithumb_url)\n",
    "    soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "    soup = soup.text.replace('{\"status\":\"0000\",\"data\":{', '')\n",
    "\n",
    "    currency = []\n",
    "    timestamp_kst = []\n",
    "    opening_price = []\n",
    "    high_price = []\n",
    "    low_price = []\n",
    "    close_price = []\n",
    "    acc_trade_value = []\n",
    "    unit_traded = []\n",
    "\n",
    "    for i in soup.split('},')[:-1]:\n",
    "        currency.append(i.split(':')[0].replace('\"', ''))\n",
    "        timestamp_kst.append(datetime.utcfromtimestamp(\n",
    "            int(soup.split('},')[-1].replace('\"', '').replace('}}', '').replace('date:', '')) / 1000).strftime(\n",
    "            '%Y-%m-%d'))\n",
    "        opening_price.append(i.split(',')[0].split(':')[2].replace('\"', ''))\n",
    "        high_price.append(i.split(',')[3].split(':')[1].replace('\"', ''))\n",
    "        low_price.append(i.split(',')[2].split(':')[1].replace('\"', ''))\n",
    "        close_price.append(i.split(',')[1].split(':')[1].replace('\"', ''))\n",
    "        acc_trade_value.append(i.split(',')[8].split(':')[1].replace('\"', ''))\n",
    "        unit_traded.append(i.split(',')[7].split(':')[1].replace('\"', ''))\n",
    "\n",
    "    bithumb_df = pd.concat([pd.DataFrame(currency, columns=['currency'])\n",
    "                               , pd.DataFrame(timestamp_kst, columns=['timestamp_kst'])\n",
    "                               , pd.DataFrame(opening_price, columns=['opening_price'])\n",
    "                               , pd.DataFrame(high_price, columns=['high_price'])\n",
    "                               , pd.DataFrame(low_price, columns=['low_price'])\n",
    "                               , pd.DataFrame(close_price, columns=['close_price'])\n",
    "                               , pd.DataFrame(acc_trade_value, columns=['acc_trade_value'])\n",
    "                               , pd.DataFrame(unit_traded, columns=['unit_traded'])]\n",
    "                           , axis=1)\n",
    "\n",
    "    bithumb_df['exchange'] = 'Bithumb'\n",
    "    print(\"Bithumb Done!\")\n",
    "\n",
    "    return bithumb_df\n",
    "\n",
    "\n",
    "\n",
    "#######################\n",
    "### Coinone Crawler ###\n",
    "#######################\n",
    "def coinone_crawler():\n",
    "    \"\"\"\n",
    "    Coinone Crawl during 24H information\n",
    "    \"\"\"\n",
    "    coinone_url = 'https://api.coinone.co.kr/ticker?currency=all'\n",
    "    html = requests.get(coinone_url)\n",
    "    soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "\n",
    "    currency = []\n",
    "    timestamp_kst = []\n",
    "    opening_price = []\n",
    "    high_price = []\n",
    "    low_price = []\n",
    "    close_price = []\n",
    "    acc_trade_value = []\n",
    "    unit_traded = []\n",
    "\n",
    "    for i in soup.text.split(':{')[1:]:\n",
    "        currency.append(i.split(',')[0].split(':')[1].replace('\"', ''))\n",
    "        timestamp_kst.append(datetime.utcfromtimestamp(\n",
    "            int(soup.text.split(':{')[0].split(',')[2].split(':')[1].replace('\"', ''))).strftime('%Y-%m-%d'))\n",
    "        opening_price.append(i.split(',')[1].split(':')[1].replace('\"', ''))\n",
    "        high_price.append(i.split(',')[3].split(':')[1].replace('\"', ''))\n",
    "        low_price.append(i.split(',')[2].split(':')[1].replace('\"', ''))\n",
    "        close_price.append(i.split(',')[4].split(':')[1].replace('\"', ''))\n",
    "        acc_trade_value.append(float(i.split(',')[4].split(':')[1].replace('\"', '')) * float(\n",
    "            i.split(',')[5].split(':')[1].replace('\"', '')))\n",
    "        unit_traded.append(i.split(',')[5].split(':')[1].replace('\"', ''))\n",
    "\n",
    "    coinone_df = pd.concat([pd.DataFrame(currency, columns=['currency'])\n",
    "                               , pd.DataFrame(timestamp_kst, columns=['timestamp_kst'])\n",
    "                               , pd.DataFrame(opening_price, columns=['opening_price'])\n",
    "                               , pd.DataFrame(high_price, columns=['high_price'])\n",
    "                               , pd.DataFrame(low_price, columns=['low_price'])\n",
    "                               , pd.DataFrame(close_price, columns=['close_price'])\n",
    "                               , pd.DataFrame(acc_trade_value, columns=['acc_trade_value'])\n",
    "                               , pd.DataFrame(unit_traded, columns=['unit_traded'])]\n",
    "                           , axis=1)\n",
    "\n",
    "    coinone_df['currency'] = [i.upper() for i in coinone_df['currency']]\n",
    "    coinone_df['exchange'] = 'Coinone'\n",
    "    print(\"Coinone Done!\")\n",
    "\n",
    "    return coinone_df\n",
    "\n",
    "\n",
    "\n",
    "######################\n",
    "### Korbit Crawler ###\n",
    "######################\n",
    "def korbit_crawler():\n",
    "    \"\"\"\n",
    "    Korbit Crawl during 24H information\n",
    "    \"\"\"\n",
    "    korbit_url = 'https://api.korbit.co.kr/v1/ticker/detailed/all'\n",
    "    html = requests.get(korbit_url)\n",
    "    soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "\n",
    "    currency = []\n",
    "    timestamp_kst = []\n",
    "    opening_price = []\n",
    "    high_price = []\n",
    "    low_price = []\n",
    "    close_price = []\n",
    "    acc_trade_value = []\n",
    "    unit_traded = []\n",
    "\n",
    "    for i in soup.text.split('},'):\n",
    "        currency.append(i.split(':')[0].replace('{', '').replace('\"', ''))\n",
    "        timestamp_kst.append(datetime.utcfromtimestamp(int(i.split(':')[2].split(',')[0]) / 1000).strftime('%Y-%m-%d'))\n",
    "        opening_price.append(i.split(',')[2].split(':')[1].replace('\"', ''))\n",
    "        high_price.append(i.split(',')[6].split(':')[1].replace('\"', ''))\n",
    "        low_price.append(i.split(',')[5].split(':')[1].replace('\"', ''))\n",
    "        close_price.append(i.split(',')[1].split(':')[1].replace('\"', ''))\n",
    "        acc_trade_value.append(float(i.split(',')[7].split(':')[1].replace('\"', '')) * float(\n",
    "            i.split(',')[1].split(':')[1].replace('\"', '')))\n",
    "        unit_traded.append(i.split(',')[7].split(':')[1].replace('\"', ''))\n",
    "\n",
    "    korbit_df = pd.concat([pd.DataFrame(currency, columns=['currency'])\n",
    "                              , pd.DataFrame(timestamp_kst, columns=['timestamp_kst'])\n",
    "                              , pd.DataFrame(opening_price, columns=['opening_price'])\n",
    "                              , pd.DataFrame(high_price, columns=['high_price'])\n",
    "                              , pd.DataFrame(low_price, columns=['low_price'])\n",
    "                              , pd.DataFrame(close_price, columns=['close_price'])\n",
    "                              , pd.DataFrame(acc_trade_value, columns=['acc_trade_value'])\n",
    "                              , pd.DataFrame(unit_traded, columns=['unit_traded'])]\n",
    "                          , axis=1)\n",
    "\n",
    "    korbit_df['currency'] = [i.split('_')[0].upper() for i in korbit_df['currency']]\n",
    "    korbit_df['exchange'] = 'Korbit'\n",
    "    print(\"Korbit Done!\")\n",
    "\n",
    "    return korbit_df\n",
    "\n",
    "\n",
    "\n",
    "#####################\n",
    "### Gopax Crawler ###\n",
    "#####################\n",
    "def gopax_crawler():\n",
    "    \"\"\"\n",
    "    Gopax Crawl during 24H information\n",
    "    \"\"\"\n",
    "    gopax_url = 'https://api.gopax.co.kr/trading-pairs/stats'\n",
    "    html = requests.get(gopax_url)\n",
    "    soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "\n",
    "    currency = []\n",
    "    timestamp_kst = []\n",
    "    opening_price = []\n",
    "    high_price = []\n",
    "    low_price = []\n",
    "    close_price = []\n",
    "    acc_trade_value = []\n",
    "    unit_traded = []\n",
    "\n",
    "    for i in soup.text.split(',{'):\n",
    "        currency.append(i.split(',')[0].split(':')[1].replace('\"', ''))\n",
    "        timestamp_kst.append(i.split(',')[6].split(':')[1].replace('\"', '').replace('T', ' '))\n",
    "        opening_price.append(i.split(',')[1].split(':')[1])\n",
    "        high_price.append(i.split(',')[2].split(':')[1])\n",
    "        low_price.append(i.split(',')[3].split(':')[1])\n",
    "        close_price.append(i.split(',')[4].split(':')[1])\n",
    "        acc_trade_value.append(float(i.split(',')[4].split(':')[1]) * float(i.split(',')[5].split(':')[1]))\n",
    "        unit_traded.append(i.split(',')[5].split(':')[1])\n",
    "\n",
    "    gopax_df = pd.concat([pd.DataFrame(currency, columns=['currency']),\n",
    "                          pd.DataFrame(timestamp_kst, columns=['timestamp_kst']),\n",
    "                          pd.DataFrame(opening_price, columns=['opening_price']),\n",
    "                          pd.DataFrame(high_price, columns=['high_price']),\n",
    "                          pd.DataFrame(low_price, columns=['low_price']),\n",
    "                          pd.DataFrame(close_price, columns=['close_price']),\n",
    "                          pd.DataFrame(acc_trade_value, columns=['acc_trade_value']),\n",
    "                          pd.DataFrame(unit_traded, columns=['unit_traded'])],\n",
    "                         axis=1)\n",
    "\n",
    "    gopax_df['currency'] = [i.split('-')[0].upper() for i in gopax_df['currency']]\n",
    "    gopax_df['exchange'] = 'Gopax'\n",
    "    print(\"Gopax Done!\")\n",
    "\n",
    "    return gopax_df\n",
    "\n",
    "\n",
    "\n",
    "#####################\n",
    "### Upbit Crawler ###\n",
    "#####################\n",
    "def upbit_crawler():\n",
    "    \"\"\"\n",
    "    Upbit Crawl during 24H information\n",
    "    first, Crawling currency-pair\n",
    "    second, Crawling 1H data\n",
    "    third, Union 1H data(1H * 24)\n",
    "    \"\"\"\n",
    "    # Crawling currency-pair\n",
    "    upbit_url = 'https://api.upbit.com/v1/market/all'\n",
    "    html = requests.get(upbit_url)\n",
    "    soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "\n",
    "    upbit_currency = soup.text.split('},')\n",
    "\n",
    "    ignore_char = '\"\"'\n",
    "\n",
    "    basis_dy = []\n",
    "    exchange = []\n",
    "    currency_pair = []\n",
    "    currency_name = []\n",
    "\n",
    "    for i in upbit_currency:\n",
    "        currency_pair.append(i.split(':')[1].split(',')[0].replace('\"', ''))\n",
    "        currency_name.append(i.split(':')[3].replace('}]', '').replace('\"', ''))\n",
    "\n",
    "    currency_pair_df = pd.DataFrame(currency_pair)\n",
    "    currency_name_df = pd.DataFrame(currency_name)\n",
    "\n",
    "    upbit_pair = pd.concat([currency_pair_df, currency_name_df], axis=1)\n",
    "    upbit_pair.columns = ['currency_pair', 'name']\n",
    "\n",
    "    upbit_pair['currency_krw'] = [i.split('-')[0] for i in upbit_pair['currency_pair']]\n",
    "    upbit_pair = upbit_pair.where(upbit_pair['currency_krw'] == 'KRW')\n",
    "    upbit_pair = upbit_pair.dropna(axis=0)\n",
    "\n",
    "    currency_m = []\n",
    "    timestamp_kst_m = []\n",
    "    opening_price_m = []\n",
    "    high_price_m = []\n",
    "    low_price_m = []\n",
    "    close_price_m = []\n",
    "    acc_trade_value_m = []\n",
    "    unit_traded_m = []\n",
    "\n",
    "    # crawling 1H data\n",
    "    for i in upbit_pair['currency_pair']:\n",
    "        upbit_price = 'https://api.upbit.com/v1/candles/minutes/60?market={}&count=24'.format(i)\n",
    "        time.sleep(0.1)\n",
    "        html = requests.get(upbit_price)\n",
    "        soup = BeautifulSoup(html.text, 'html.parser')\n",
    "        soup = soup.text.split(',{')\n",
    "\n",
    "        for j in soup:\n",
    "            currency_m.append(j.split(',')[0].split(':')[1].replace('\"', ''))\n",
    "            timestamp_kst_m.append(j.split(',')[1].split(':')[1].replace('T', ' ').replace('\"', ''))\n",
    "            opening_price_m.append(j.split(',')[3].split(':')[1])\n",
    "            high_price_m.append(j.split(',')[4].split(':')[1])\n",
    "            low_price_m.append(j.split(',')[5].split(':')[1])\n",
    "            close_price_m.append(j.split(',')[6].split(':')[1])\n",
    "            acc_trade_value_m.append(j.split(',')[8].split(':')[1])\n",
    "            unit_traded_m.append(j.split(',')[9].split(':')[1])\n",
    "\n",
    "    upbit_df_m = pd.concat([pd.DataFrame(currency_m, columns=['currency_m']),\n",
    "                            pd.DataFrame(timestamp_kst_m, columns=['timestamp_kst_m']),\n",
    "                            pd.DataFrame(opening_price_m, columns=['opening_price_m']),\n",
    "                            pd.DataFrame(high_price_m, columns=['high_price_m']),\n",
    "                            pd.DataFrame(low_price_m, columns=['low_price_m']),\n",
    "                            pd.DataFrame(close_price_m, columns=['close_price_m']),\n",
    "                            pd.DataFrame(acc_trade_value_m, columns=['acc_trade_value_m']),\n",
    "                            pd.DataFrame(unit_traded_m, columns=['unit_traded_m'])],\n",
    "                           axis=1)\n",
    "\n",
    "    upbit_df_m['acc_trade_value_m'] = upbit_df_m['acc_trade_value_m'].astype(float)\n",
    "    upbit_df_m['unit_traded_m'] = upbit_df_m['unit_traded_m'].astype(float)\n",
    "\n",
    "    # Union 1H data\n",
    "    currency = []\n",
    "    timestamp_kst = []\n",
    "    opening_price = []\n",
    "    high_price = []\n",
    "    low_price = []\n",
    "    close_price = []\n",
    "    acc_trade_value = []\n",
    "    unit_traded = []\n",
    "\n",
    "    currency.append(upbit_df_m.groupby('currency_m').max().reset_index(drop=False)['currency_m'])\n",
    "    timestamp_kst.append(upbit_df_m.groupby('currency_m').max()['timestamp_kst_m'])\n",
    "    opening_price.append(upbit_df_m.groupby('currency_m').min()['opening_price_m'])\n",
    "    high_price.append(upbit_df_m.groupby('currency_m').max()['high_price_m'])\n",
    "    low_price.append(upbit_df_m.groupby('currency_m').min()['low_price_m'])\n",
    "    close_price.append(upbit_df_m.groupby('currency_m').max()['close_price_m'])\n",
    "    acc_trade_value.append(upbit_df_m.groupby('currency_m').sum('acc_trade_value_m')['acc_trade_value_m'])\n",
    "    unit_traded.append(upbit_df_m.groupby('currency_m').sum('unit_traded_m')['unit_traded_m'])\n",
    "\n",
    "    upbit_df = pd.concat([pd.DataFrame(currency).T.set_index('currency_m'),\n",
    "                          pd.DataFrame(timestamp_kst).T,\n",
    "                          pd.DataFrame(opening_price).T,\n",
    "                          pd.DataFrame(high_price).T,\n",
    "                          pd.DataFrame(low_price).T,\n",
    "                          pd.DataFrame(close_price).T,\n",
    "                          pd.DataFrame(acc_trade_value).T,\n",
    "                          pd.DataFrame(unit_traded).T],\n",
    "                         axis=1)\n",
    "\n",
    "    upbit_df = upbit_df.reset_index(drop=False)\n",
    "    upbit_df.columns = ['currency', 'timestamp_kst', 'opening_price', 'high_price', 'low_price', 'close_price',\n",
    "                        'acc_trade_value', 'unit_traded']\n",
    "\n",
    "    upbit_df['currency'] = [i.split('-')[1] for i in upbit_df['currency']]\n",
    "    upbit_df['exchange'] = 'Upbit'\n",
    "    print(\"Upbit Done!\")\n",
    "\n",
    "    return upbit_df\n",
    "\n",
    "\n",
    "\n",
    "#####################\n",
    "### Summary Table ###\n",
    "#####################\n",
    "def MakeDataFrame(upbit_df, bithumb_df, coinone_df, korbit_df, gopax_df):\n",
    "    \"\"\"\n",
    "    Make Total DataFrame\n",
    "    \"\"\"\n",
    "    total_df = pd.concat([bithumb_df, coinone_df, korbit_df, upbit_df, gopax_df], axis=0)\n",
    "    total_df.columns = ['base_currency', 'etime_date', 'open_price', 'high_price', 'low_price', 'close_price',\n",
    "                        'krw_volume', 'base_volume', 'exchange']\n",
    "    total_df = total_df[\n",
    "        ['exchange', 'base_currency', 'open_price', 'high_price', 'low_price', 'close_price', 'krw_volume',\n",
    "         'base_volume', 'etime_date']]\n",
    "    total_df = total_df.reset_index(drop=True)\n",
    "    print(total_df)\n",
    "    print(\"All Done!\")\n",
    "    logging.info(\"-------------sys.argv list-------------\", print(sys.argv), sys.argv)\n",
    "\n",
    "    # return total_df # Original Method(On docker-compose airflow or local airflow)\n",
    "    return total_df.to_csv('/Users/yoo/Data-dev/nlp/dev/price.csv') # Temporary save DataFrame\n",
    "\n",
    "\n",
    "\n",
    "##############################\n",
    "### Upload DataFrame to S3 ###\n",
    "##############################\n",
    "# def DataFrameToS3Upload(total_df):\n",
    "#     \"\"\"\n",
    "#     Upload DataFrame(df) to S3 bucket\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         result = total_df.to_dict()\n",
    "#         results_string = str(total_df.T.to_dict().values()).replace(\"dict_values([\", '').replace('])', '').replace(\"'\",\n",
    "#                                                                                                                    '\"').replace(\n",
    "#             ' ', '').replace('},{', '}{')\n",
    "#         logging.info(results_string[:51])\n",
    "#         logging.info('******************* Changing JSON FORMAT to Bytes *******************')\n",
    "#         results_bytes = results_string.encode('utf-8')\n",
    "#         logging.info('******************* gzip the results *******************')\n",
    "#         results_gzip = gzip.compress(results_bytes)\n",
    "#         results = results_gzip\n",
    "\n",
    "#         dag_id = 'price_crawler_v0001'\n",
    "\n",
    "#         s3_key = '/airflow/dags' + dag_id + str((datetime.today() + timedelta(days=-1)).strftime('%Y%m%d')).replace('-', '') + '.json.gzip'\n",
    "#         s3_hook = AwsHook(aws_conn_id='{{ params.s3_conn_id }}')\n",
    "#         s3_client_type = s3_hook.get_client_type(client_type='s3', region_name='ap-northeast-1')\n",
    "#         s3 = s3_client_type\n",
    "#         s3.put_object(\n",
    "#             Bucket=s3_bucket,\n",
    "#             Body=results,\n",
    "#             Key=s3_key\n",
    "#         )\n",
    "#         print(\"--------------- Python process end ---------------\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(\"S3 uploader Error\", e)\n",
    "\n",
    "#     return results\n",
    "\n",
    "\n",
    "####################\n",
    "### Main Complie ###\n",
    "####################\n",
    "df = pd.DataFrame(MakeDataFrame(\n",
    "    upbit_df=upbit_crawler(),\n",
    "    bithumb_df=bithumb_crawler(),\n",
    "    coinone_df=coinone_crawler(),\n",
    "    korbit_df=korbit_crawler(),\n",
    "    gopax_df=gopax_crawler()))\n",
    "\n",
    "print(df)\n",
    "\n",
    "# DataFrameToS3Upload(df)\n",
    "# s3_bucket = 'russo-mydata'\n",
    "# s3_hook = S3Hook(s3_bucket)\n",
    "#\n",
    "#\n",
    "# #S3 디렉토리 리스팅\n",
    "# keys = s3_hook.list_keys(bucket_name=bucket)\n",
    "#\n",
    "# # s3에 업로드\n",
    "# s3_key = 'airflow/price.csv'\n",
    "# s3_hook.load_file(filename='/var/lib/airflow/test/test.csv',key=s3_key,bucket_name=s3_bucket)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
