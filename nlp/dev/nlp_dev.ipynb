{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T05:25:19.890209Z",
     "start_time": "2021-09-22T05:24:31.820838Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Extracting Web page..:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yoo/opt/anaconda3/bin/python\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Web page..: 100%|██████████| 20/20 [00:24<00:00,  1.24s/it]\n",
      "Extracting News url..: 100%|██████████| 80/80 [00:00<00:00, 30302.93it/s]\n",
      "Crawling News article..: 100%|██████████| 80/80 [00:22<00:00,  3.58it/s]\n",
      "Crawling News datetime..: 100%|██████████| 80/80 [00:00<00:00, 127003.91it/s]\n",
      "processing complie 1: 100%|██████████| 80/80 [00:00<00:00, 8495.22it/s]\n",
      "processing complie 2: 100%|██████████| 80/80 [00:00<00:00, 28503.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make DataFrame Structure\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "print(sys.executable)\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "\n",
    "### Use list ###\n",
    "basic_date_list = []\n",
    "basic_news_article = []\n",
    "depth_date_list = []\n",
    "depth_news_article = []\n",
    "clear_word = []\n",
    "\n",
    "\n",
    "### Crawler 1 ###\n",
    "def basic_crawler():\n",
    "    \"\"\"\n",
    "    Crawler(data gathering)\n",
    "    \"\"\"\n",
    "    d = []\n",
    "    a = []\n",
    "\n",
    "    for i in tqdm(range(1,21), desc=\"Extracting Web page..\"):\n",
    "        url = 'https://www.coindeskkorea.com/news/articleList.html?page={}&total=6048&box_idxno=&view_type=sm'.format(i)\n",
    "        resp = requests.get(url)\n",
    "        soup = BeautifulSoup(resp.content, 'lxml')\n",
    "\n",
    "        for j in range(1,21):\n",
    "            date = soup.select('#user-container div.float-center.custom-m.mobile.template.list.max-width-1250 div.user-content section article div.article-list section div:nth-of-type({}) div.text-block div.list-dated'.format(j))\n",
    "            article = soup.select(\n",
    "            '#user-container div.float-center.custom-m.mobile.template.list.max-width-1250 div.user-content section article div.article-list section div:nth-of-type({}) div.text-block p a'.format(\n",
    "                j))\n",
    "\n",
    "            d.append(date)\n",
    "            a.append(article)\n",
    "\n",
    "    for _ in d:\n",
    "        if _:\n",
    "            basic_date_list.append(_)\n",
    "\n",
    "    for _ in a:\n",
    "        if _:\n",
    "            basic_news_article.append(_)\n",
    "\n",
    "    return basic_date_list, basic_news_article\n",
    "\n",
    "\n",
    "### Crawler 2 ###\n",
    "def depth_crawler():\n",
    "    \"\"\"\n",
    "    Optimization basic_crawl data\n",
    "    \"\"\"\n",
    "    # depth_news_article\n",
    "    processing_news = []\n",
    "    for i in tqdm(basic_news_article, desc=\"Extracting News url..\"):\n",
    "        url = 'https://www.coindeskkorea.com'+str(i).split(' ')[2].split('\"')[1]\n",
    "\n",
    "        processing_news.append(url)\n",
    "\n",
    "    for i in tqdm(processing_news, desc=\"Crawling News article..\"):\n",
    "        resq = requests.get(i)\n",
    "        soup = BeautifulSoup(resq.text,'lxml')\n",
    "        n = str(soup.find_all('p')).split('<p><a href=')[0]\n",
    "\n",
    "        depth_news_article.append(n)\n",
    "\n",
    "    # depth_date_list\n",
    "    for i in tqdm(basic_date_list, desc=\"Crawling News datetime..\"):\n",
    "        d = i[0].text.split(' ')[-2]\n",
    "\n",
    "        depth_date_list.append(d)\n",
    "\n",
    "    return depth_news_article, depth_date_list\n",
    "\n",
    "\n",
    "### pre-processing ###\n",
    "def processing():\n",
    "    \"\"\"\n",
    "    Article pre-processing\n",
    "    \"\"\"\n",
    "    process = []\n",
    "    for i in tqdm(depth_news_article, desc=\"processing complie 1\"):\n",
    "        a = re.compile('[가-힣]+').findall(i)\n",
    "\n",
    "        process.append(a)\n",
    "\n",
    "    for i in tqdm(process, desc=\"processing complie 2\"):\n",
    "        a = ' '.join(i)\n",
    "        b = re.compile('[제보-보내주세요]').sub(' ',a)\n",
    "        # c = b.replace('도자료','')\n",
    "\n",
    "        clear_word.append(' '+b)\n",
    "\n",
    "    return clear_word\n",
    "\n",
    "\n",
    "### dataframe ###\n",
    "def dataframe():\n",
    "    df_article = pd.DataFrame(clear_word, columns=['article'])\n",
    "    df_date = pd.DataFrame(depth_date_list, columns=['created_date'])\n",
    "    df_total = pd.concat([df_date, df_article], axis=1)\n",
    "    df_total = df_total.groupby('created_date').sum(str(df_total['article']))\n",
    "    print(\"Make DataFrame Structure\")\n",
    "\n",
    "    return df_total.to_csv('/Users/yoo/Data-dev/nlp/reference/article/df_total.csv')\n",
    "\n",
    "\n",
    "basic_crawler()\n",
    "depth_crawler()\n",
    "processing()\n",
    "dataframe()\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
